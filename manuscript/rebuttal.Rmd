---
title: "Response to reviewer comments (The five pillars)"
author: "Mark Ziemann, Pierre Poulain, Anusuiya Bora"
date: "`r Sys.Date()`"
output: word_document
code_folding: hide
fig_width: 7
fig_height: 6
bibliography: 5pillars.bib
csl: plos-computational-biology.csl
---

Dear Editor,

Many thanks for your consideration of our manuscript.
Below is our point-by-point response to reviewer comments.

### Reviewer 1

> In this manuscript, the authors present a framework that computational researchers should follow to
> maximize the reproducibility of their computational research. The five pillars that they identified
> are 1) code version control and sharing, 2) compute environment control,  3) literate programming,
> 4) documentation and 5) FAIR data sharing. They give a fair amount of detail regarding each pillar.
> This paper contains no original research and is more of an opinion piece. With that in mind, if BiB
> is interested in this topic, I recommend publishing. The authors make very reasonable suggestions.
> My only request for edits is to rearrange the section structure of the manuscript to be in line with
> the numbering of the pillars. Right now it is quite confusing.

Response: We thank reviewer 1 for their positive feedback.
With regards to the ordering of the five pillars, we have amended Figure 1 so that the pillars now
appear in the same order as they are detailed in sections following.

### Reviewer 2

> Comments to the Author: The article offers a comprehensive perspective on reproducibility in
> computational research, delivering clear and logically backed recommendations. It is indeed a very
> interesting topic that needs our attention. Nevertheless, there are areas that could benefit from
> further development and clarity:

Response: We appreciate reviewer 2's positive feedback, insights and thoughtful suggestions.

> 1. Inclusion of Case Studies or Examples: To strengthen your argument, consider incorporating case
> studies or practical examples that demonstrate the effectiveness of your recommendations.
> Highlighting specific instances where these best practices have enhanced reproducibility or solved
> particular issues could substantiate your points more effectively.

To demonstrate the importance of computational reprodcibility, we elaborate on the
"forensic bioinformatics" case study of Baggerly and Coombes (2010) critique of Potti et al (2006)
in the introduction:

> For example, in 2006 an article entitled "Genomic signatures to guide the use of chemotherapeutics"
> generated a great deal of interest as it was an early application of high throughput transcriptomics
> in the prediction of individual patient responses to different chemotherapies @Potti2006-yk.
> After observing some unusual features of the patient group, Baggerly and Coombes attempted
> reproduction of some of the key findings @Baggerly2010-nr.
> Without the help of scripted workflows to guide the re-analysis, the team used forensic bioinformatic
> techniques to piece together how the dataset was originally analysed.
> Their investigations found a litany of issues.
> Firstly, the labeling of patients in the test set as "responders" or "non-responders" had been reversed
> in the original analysis.
> Secondly, some of the patients were included more than once (some up to four times) in the analysis,
> likely to cause major distortions in results.
> Confusingly, some of the reused data has inconsistent grouping; ie: some of the samples are labeled
> both sensitive and resistant.
> Additional errors include two cases where results (charts) were ascribed to the wrong drug.
> Baggerly and Coombes highlight that such mistakes can inadvertently occur when conducting unscripted
> data analysis such as using spreadsheets, and such problems can be obscured by a lack of documentation.
> The article underwent two corrigenda, but was untimately retracted in 2011, as the authors were not
> able to reproduce the findings themselves due to "corruption of several validation data sets"
> @Potti2011-ut.
> As the array findings were the basis for clinical trials where patients were allocated to treatments,
> the flawed data analysis may have harmed patients given the wrong drug in the
> period 2007-2010.
> In 2010, Duke University terminated the trials and suspended the lead author, Dr Anil Potti, who
> later resigned.
> Duke was served eight lawsuits by families of affected patients seeking compensation for exposure to
> harmful and unnecessary chemotherapy, which were settled out of court @Dyer2015-my.
> This worst-case scenario emphasizes that computational reproducibility is crucial for
> translating bioinformatics research into real-world outcomes.

> 2. Expanding on Current Shortcomings: The article mentions that current practices are not meeting the
> goal of reproducibility. An expanded discussion on the ramifications of these shortcomings on
> scientific progress could accentuate the urgency and significance of addressing this issue.

In the introduction we have spoken to the consequences of poor computational research, and emphasize
the urgency of this issue (which complements the case study summary above):

> The ramifications of irreproducible and unreliable research include misleading the community,
> wasting research funds, slowing scientific progress, eroding public confidence in science and
> tarnishing the reputation of associated institutions and colleagues.
> In certain cases, irreproducible bioinformatics has the potential to place patient safety at risk.

> 3. Focus on Implementation Challenges: While the article acknowledges the importance of certain
> practices, an increased focus on potential obstacles researchers may face when trying to implement
> these suggestions, as well as strategies to overcome these challenges, would be insightful.

We have re-written the Challenges section to highlight some of the barriers slowing the
adoption of best practices, with reference to previous contributions on the topic.
These are structured around what different stakeholders can do to improve the situation (including
funders, journals, institutions and individual researchers).

> 4. Clear Structure: Consider organizing the article into clearly defined sections such as
> 'Introduction', 'Background', 'Recommendations', 'Challenges', and 'Conclusion'.
> This could significantly enhance readability and comprehensibility.

The subheading structure has been amended accordingly.

* Abstract

* Introduction

* Recommendations

  * End-to-end automated process

  * Literate programming

  * Code version control and persistent sharing

  * Compute environment control

  * FAIR and persistent data sharing

  * Documentation

* Challenges

* Conclusion

* Key points

> 5. Engaging Conclusion: Reworking the conclusion to be more impactful could be beneficial. Instead of
> merely summarizing the recommendations, it would be useful to indicate the potential trajectory of
> computational research if these recommendations are adopted, as well as highlighting the potential
> risks if they aren't.

Response: We have completely rewritten the conclusion.
Instead of reiterating the five pillars, we focus on the motivation for adoption:

> Large scale datasets are becoming commonplace in medicine and life sciences thanks to the commoditisation
> of platform technologies like DNA sequencing, mass spectrometry and advanced imaging systems.
> These big datasets are enabling breakthrough discoveries, but due to their size and complexity, require
> significant expertise to analyze and distill key observations.
> The measures proposed here are designed to enhance transparency and foster the reuse of data and code;
> reducing research waste.
> Transparency makes it easier for problems to be identified, so that erroneous findings are less likely to
> pollute the scientific literature.
> Researchers, their institutions, publishers and funders each have a major role to play in fighting the
> reproducibility crisis by encouraging highly reproducible research practices.

## Reviewer 3

> The paper is mostly well written and addresses an important topic: computational reproducibility and
> tools to achieve it. The authors describe five "pillars" under which various tools and techniques can
> be categorized. The paper is pretty comprehensive in what it covers and has useful supplementary
> material. My main issue with the paper is that it reiterates many things that have been covered in
> other papers. The authors cite 11 such papers. Many of the topics covered in submitted paper have
> already been covered well in other papers. I am most familiar with reference 13, which covers many of
> the same topics, although this paper goes into more detail on many issues. The authors could do more
> to differentiate their paper from previous ones and perhaps remove some topics that are already
> covered well elsewhere.

RESPONSE: XX

> Aside from that, I have listed below some relatively minor issues that, if addressed, would improve the paper. When I list page numbers, I am using the PDF page numbers rather than the numbers shown in the top-left corner of the manuscript.

> 2. Page 3, Line 36: "bioinformatics data analysts (not tool developers)".
> The authors state these individuals as the primary audience, but some parts of the paper seem to be
> targeted at a more technical audience. Or maybe I am misunderstanding the intent. If the audience
> is "bioinformatics data analysts," that would imply people who are bioinformaticians but are
> analyzing data rather than creating tools. However, a much bigger audience (and perhaps more
> important audience) are non-bioinformaticians who analyze data.

RESPONSE: The passage has been amended for clarity:

> The intended audience is anyone who analyzes biological data.

> 3. Page 3, Line 43: "enshrined by code" (this language is awkward)

RESPONSE: The passage has been amended for clarity:

> "... it needs to be formalised in code and ... "

> 4. Page 3, Lines 45-47: What about tasks that cannot be automated? U see that this topic is addressed later. But this part implies that everything can be automated.

> 5. Page 3, Line 52: It says that spreadsheets are "overused and misused." This is subjective and not backed by evidence, other than the well-known examples of gene symbols being formatted as dates.

> 6. Page 3, Line 56: It is not necessarily true that analyses performed using web tools are not reproducible. Although rare, some web tools facilitate reproducibility by providing code or configuration files and/or allowing the apps to be executed locally.

> 7. Page 5, Line 15, type-o = "authors provided along the"

> 8. Page 6, Line 21: "quantum leap" (this term is overly optimistic in this context)

> 9. How do you get from a notebook to an actual paper submission if you have to do custom formatting of the document, including references? My understanding is that this is still not possible, but please correct me if I'm wrong.

> 10. What about when your data files are too large to fit on a personal computer?

> 11. What about computationally intensive tasks that must be performed using specialized computing environments like the cloud or clusters?

> 12. Page 6, line 32: The master script idea was already mentioned earlier.

> 13. Section on version control: This section focuses mostly on using VC for software development (that is my interpretation). To be consistent with the introduction, it should focus more on data analyses. Although I use VC for analyses, I feel that simpler approaches are better in many cases. For example Dropbox and Google Drive provide some version-control and backup functionality and do not require the same level of knowledge as git.

> 14. Page 7: Many data analysts will not know what JupyterLab or VS Code are. References are also needed.

> 15. Page 9, line 7: References are missing for these other tools.

> 16. Page 9, lines 7-8: I believe you, but I am not aware of evidence that supports this claim.

> 17. The Biocontainers project should be mentioned.

> 18. Page 10, lines 51-52: I disagree that the risk is small. There are many instances of using genomic data to identify individuals who have committed crimes.

> 19. Figures 2 and 4 are very similar to figures used in reference 13.

> 20. Figure 5: I don't think it's really necessary to resummarize the FAIR principles. People can go to the source article for that.

> 21. Page 11, line 13: I disagree that repositories like GEO and SRA are FAIR. There are lots of problems with FAIRness in these repositories.

> 22. Page 11, lines 13-15: That's not true for some of these disciplines. Ecology has NEON, evo bio has NCBI.

> 23. Page 11, line 34: Need evidence to back this up.

> 24. Page 11, line 39: It is not necessarily true that CSV files are better than Excel. Excel can retain information about data types, for example, whereas CSVs do not. It depends on what you are trying to accomplish.

> 25. One thing that could be added is something about Common Workflow Language. It's a community-supported specification for accomplishing many of the objectives described here. There are some recent papers about this.

> 26. Page 14, lines 9-10: The big question is how to make more progress. We have the tools to achieve reproducibility, but why are we rarely achieving it? The paper mentions incentives and lack of training, which are true. You might consider elaborating a bit. My cynical view is that writing more papers and tutorials will do little without strong incentives and more automation.
