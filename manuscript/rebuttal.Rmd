---
title: "Response to reviewer comments (The five pillars)"
author: "Mark Ziemann, Pierre Poulain, Anusuiya Bora"
date: "`r Sys.Date()`"
output: word_document
code_folding: hide
fig_width: 7
fig_height: 6
bibliography: 5pillars.bib
csl: plos-computational-biology.csl
---

Dear Editor,

Many thanks for your consideration of our manuscript.
Below is our point-by-point response to reviewer comments.

## Reviewer 1

> In this manuscript, the authors present a framework that computational researchers should follow to
> maximize the reproducibility of their computational research. The five pillars that they identified
> are 1) code version control and sharing, 2) compute environment control,  3) literate programming,
> 4) documentation and 5) FAIR data sharing. They give a fair amount of detail regarding each pillar.
> This paper contains no original research and is more of an opinion piece. With that in mind, if BiB
> is interested in this topic, I recommend publishing. The authors make very reasonable suggestions.
> My only request for edits is to rearrange the section structure of the manuscript to be in line with
> the numbering of the pillars. Right now it is quite confusing.

Response: We thank reviewer 1 for their positive feedback.
With regards to the ordering of the five pillars, we have amended Figure 1 so that the pillars now
appear in the same order as they are detailed in sections following.

## Reviewer 2

> Comments to the Author: The article offers a comprehensive perspective on reproducibility in
> computational research, delivering clear and logically backed recommendations. It is indeed a very
> interesting topic that needs our attention. Nevertheless, there are areas that could benefit from
> further development and clarity:

Response: We appreciate reviewer 2's positive feedback, insights and thoughtful suggestions.

### R2 Point 1

> 1. Inclusion of Case Studies or Examples: To strengthen your argument, consider incorporating case
> studies or practical examples that demonstrate the effectiveness of your recommendations.
> Highlighting specific instances where these best practices have enhanced reproducibility or solved
> particular issues could substantiate your points more effectively.

RESPONSE: To demonstrate the importance of computational reprodcibility, we elaborate on the
"forensic bioinformatics" case study of Baggerly and Coombes (2010) critique of Potti et al (2006)
in the introduction:

> For example, in 2006 an article entitled "Genomic signatures to guide the use of chemotherapeutics"
> generated a great deal of interest as it was an early application of high throughput transcriptomics
> in the prediction of individual patient responses to different chemotherapies @Potti2006-yk.
> After observing some unusual features of the patient group, Baggerly and Coombes attempted
> reproduction of some of the key findings @Baggerly2010-nr.
> Without the help of scripted workflows to guide the re-analysis, the team used forensic bioinformatic
> techniques to piece together how the dataset was originally analysed.
> Their investigations found a litany of issues.
> Firstly, the labeling of patients in the test set as "responders" or "non-responders" had been reversed
> in the original analysis.
> Secondly, some of the patients were included more than once (some up to four times) in the analysis,
> likely to cause major distortions in results.
> Confusingly, some of the reused data has inconsistent grouping; ie: some of the samples are labeled
> both sensitive and resistant.
> Additional errors include two cases where results (charts) were ascribed to the wrong drug.
> Baggerly and Coombes highlight that such mistakes can inadvertently occur when conducting unscripted
> data analysis such as using spreadsheets, and such problems can be obscured by a lack of documentation.
> The article underwent two corrigenda, but was untimately retracted in 2011, as the authors were not
> able to reproduce the findings themselves due to "corruption of several validation data sets"
> @Potti2011-ut.
> As the array findings were the basis for clinical trials where patients were allocated to treatments,
> the flawed data analysis may have harmed patients given the wrong drug in the
> period 2007-2010.
> In 2010, Duke University terminated the trials and suspended the lead author, Dr Anil Potti, who
> later resigned.
> Duke was served eight lawsuits by families of affected patients seeking compensation for exposure to
> harmful and unnecessary chemotherapy, which were settled out of court @Dyer2015-my.
> This worst-case scenario emphasizes that computational reproducibility is crucial for
> translating bioinformatics research into real-world outcomes.

### R2 Point 2

> 2. Expanding on Current Shortcomings: The article mentions that current practices are not meeting the
> goal of reproducibility. An expanded discussion on the ramifications of these shortcomings on
> scientific progress could accentuate the urgency and significance of addressing this issue.

In the introduction we have spoken to the consequences of poor computational research, and emphasize
the urgency of this issue (which complements the case study summary above):

> The ramifications of irreproducible and unreliable research include misleading the community,
> wasting research funds, slowing scientific progress, eroding public confidence in science and
> tarnishing the reputation of associated institutions and colleagues.
> In certain cases, irreproducible bioinformatics has the potential to place patient safety at risk.

### R2 Point 3

> 3. Focus on Implementation Challenges: While the article acknowledges the importance of certain
> practices, an increased focus on potential obstacles researchers may face when trying to implement
> these suggestions, as well as strategies to overcome these challenges, would be insightful.

We have re-written the Challenges section to highlight some of the barriers slowing the
adoption of best practices, with reference to previous contributions on the topic.
These are structured around what different stakeholders can do to improve the situation (including
funders, journals, institutions and individual researchers).

### R2 Point 4

> 4. Clear Structure: Consider organizing the article into clearly defined sections such as
> 'Introduction', 'Background', 'Recommendations', 'Challenges', and 'Conclusion'.
> This could significantly enhance readability and comprehensibility.

The subheading structure has been amended accordingly.

* Abstract

* Introduction

* Recommendations

  * End-to-end automated process

  * Literate programming

  * Code version control and persistent sharing

  * Compute environment control

  * FAIR and persistent data sharing

  * Documentation

* Challenges

* Conclusion

* Key points

### R2 Point 5

> 5. Engaging Conclusion: Reworking the conclusion to be more impactful could be beneficial. Instead of
> merely summarizing the recommendations, it would be useful to indicate the potential trajectory of
> computational research if these recommendations are adopted, as well as highlighting the potential
> risks if they aren't.

Response: We have completely rewritten the conclusion.
Instead of reiterating the five pillars, we focus on the motivation for adoption:

> Large scale datasets are becoming commonplace in medicine and life sciences thanks to the commoditisation
> of platform technologies like DNA sequencing, mass spectrometry and advanced imaging systems.
> These big datasets are enabling breakthrough discoveries, but due to their size and complexity, require
> significant expertise to analyze and distill key observations.
> The measures proposed here are designed to enhance transparency and foster the reuse of data and code;
> reducing research waste.
> Transparency makes it easier for problems to be identified, so that erroneous findings are less likely to
> pollute the scientific literature.
> Researchers, their institutions, publishers and funders each have a major role to play in fighting the
> reproducibility crisis by encouraging highly reproducible research practices.

## Reviewer 3

### R3 Point 1

> The paper is mostly well written and addresses an important topic: computational reproducibility and
> tools to achieve it. The authors describe five "pillars" under which various tools and techniques can
> be categorized. The paper is pretty comprehensive in what it covers and has useful supplementary
> material. My main issue with the paper is that it reiterates many things that have been covered in
> other papers. The authors cite 11 such papers. Many of the topics covered in submitted paper have
> already been covered well in other papers. I am most familiar with reference 13, which covers many of
> the same topics, although this paper goes into more detail on many issues. The authors could do more
> to differentiate their paper from previous ones and perhaps remove some topics that are already
> covered well elsewhere.

RESPONSE: We thank reviewer 3 for their careful assessment of our article and appreciate the suggestions
which have improved the manuscript.
Indeed, this article does reiterate a great deal of what has already been said, it is a review after
all.
We note that each of those recommendation articles mentioned have different emphases.
For example, some of them don't speak to the benefits of literate programming,
while others do not mention the importance of documentation.
Some are focused on specific concepts like infrastructure, containerisation or
continuous analysis.
Some are proof of principle workflows, which present a rigid combination of tools.
Here, we describe the ways computational researchers can make their own work more reproducible.
In addition to the Five Pillars infographic, the specific combination of recommendations
is unique.
And ours is the only such article that provides a handy list of recommended tutorials as a supplementary
resource.

> Aside from that, I have listed below some relatively minor issues that, if addressed, would improve
> the paper. When I list page numbers, I am using the PDF page numbers rather than the numbers shown in
> the top-left corner of the manuscript.

### R3 Point 2

> 2. Page 3, Line 36: "bioinformatics data analysts (not tool developers)".
> The authors state these individuals as the primary audience, but some parts of the paper seem to be
> targeted at a more technical audience. Or maybe I am misunderstanding the intent. If the audience
> is "bioinformatics data analysts," that would imply people who are bioinformaticians but are
> analyzing data rather than creating tools. However, a much bigger audience (and perhaps more
> important audience) are non-bioinformaticians who analyze data.

RESPONSE: The passage has been amended for clarity:

> The intended audience is anyone who analyzes biological data.

### R3 Point 3

> 3. Page 3, Line 43: "enshrined by code" (this language is awkward)

RESPONSE: The passage has been amended:

> "... it needs to be formalised in code and ... "

### R3 Point 4

> 4. Page 3, Lines 45-47: What about tasks that cannot be automated? I see that this topic is addressed
later. But this part implies that everything can be automated.

RESPONSE: The section has been amended to acknowledge that not all steps can be automated.

> To ensure reproducibility of bioinformatics workflows, they need to be formalised in code wherever
> possible, from inspecting the raw data to generating the outputs that form the conclusions of the study.

### R3 Point 5

> 5. Page 3, Line 52: It says that spreadsheets are "overused and misused." This is subjective and not
> backed by evidence, other than the well-known examples of gene symbols being formatted as dates.

RESPONSE: The passage in question has been replaced, with reference to evidence from a large
survey that found that ~69% of university academics used MS Excel in their research activities.

> Spreadsheet errors could be widespread, given that it is used as an analysis tool by ~69% of
> researchers according to a survey undertaken in 2015-16 of 20,000 university academics @Kramer2016-fj.

### R3 Point 6

> 6. Page 3, Line 56: It is not necessarily true that analyses performed using web tools are not
> reproducible. Although rare, some web tools facilitate reproducibility by providing code or
> configuration files and/or allowing the apps to be executed locally.

RESPONSE: Thank you for raising this point. We have amended the section in question:

> While webtools are valuable for data exploration, there are worries that they undermine
> reproducibility for the sake of convenience [@Wijesooriya2022-ly].
> Transferring data between compute platforms is also discouraged.
> For example, having workflows that involve combinations of web-based and scripted tools require
> data transfer steps which are inefficient and error-prone.
> On the other hand, some web based tools excel at reproducibility.
> The web-based analysis platforms Galaxy and GenePattern enable sophisticated point-and-click
> bioinformatics analysis in the browser, and those workflows can also be shared in a reproducible way
> [@The_Galaxy_Community2022-dt;@Reich2006-na].
> Some webtools facilitate reproducibility by providing code (e.g.: Degust @Powell2019-dw)
> or by allowing apps to be executed locally (e.g.: ShinyGO @Ge2020-qe).

### R3 Point 7

> 7. Page 5, Line 15, type-o = "authors provided along the"

RESPONSE: The passage in question has been amended:

> In this work, authors provided alongside the traditional research paper a Jupyter notebook ..

### R3 Point 8

> 8. Page 6, Line 21: "quantum leap" (this term is overly optimistic in this context)

RESPONSE: The passage in question has been amended:

> These features make literate programming a useful tool for science communication in a range of
> situations.

### R3 Point 9

> 9. How do you get from a notebook to an actual paper submission if you have to do custom formatting
> of the document, including references?
> My understanding is that this is still not possible, but please correct me if I'm wrong.

RESPONSE: It is possible to achieve accurate journal-specific formatting of references with a Rmarkdown
document.
It requires a few things:

* A .bib file that contains the bibliography in bibtex format.

* A .csl file that specifies the citation style.
These are avaialble from [GitHub](https://github.com/citation-style-language/styles).

* Modifcation to the Rmd header section that specifies the path of the bib and csl files.

This enables convenient reformatting of the bibliography for different venues.
In terms of overall document formatting, the `rticles` package provides a suite of
custom R Markdown LaTeX formats suitable for many journals.

We have amended the Literate Programming section to mention these features briefly:

> Quarto and R Markdown have the ability to automatically generate a bibliography in many different
> journal styles [@noauthor_undated-pg].
> Further, thanks to the `rticles` package, dozens of journal style R Markdown document templates are
> freely available @Xie2018-rf.

### R3 Point 10

> 10. What about when your data files are too large to fit on a personal computer?

RESPONSE: When data files are too large for a PC, it is common to use high performance computers, shared
clusters or cloud computers.
These computers are typically accessed via SSH (headless mode), so Quarto and Rstudio graphical
interfaces might not be available.
In those cases, the commands `quarto render document1.qmd` and `rmarkdown::render("document2.Rmd")` will
work for Quarto and R Markdown respectively.

### R3 Point 11

> 11. What about computationally intensive tasks that must be performed using specialized computing
> environments like the cloud or clusters?

RESPONSE: We have amended the relevant section to suggest a specific workflow manager for compute
intensive applications.

> Projects involving computationally intensive tasks might benefit from a build/workflow automation
> solution like `make`, `snakemake` [@Koster2012-sq; @Molder2021-mp], `targets` [@Landau2021] or
> `nextflow` [@Di_Tommaso2017-yo].

### R3 Point 12

> 12. Page 6, line 32: The master script idea was already mentioned earlier.

RESPONSE: True. We have amended the "Documentation" section to remove the unnecessary reiteration:

> As outlined above, one of the goals is to reduce the complexity of reproduction, in particular by
> minimising the number of commands required.

### R3 Point 13

> 13. Section on version control: This section focuses mostly on using VC for software development
> (that is my interpretation).
> To be consistent with the introduction, it should focus more on data analyses.
> Although I use VC for analyses, I feel that simpler approaches are better in many cases.
> For example Dropbox and Google Drive provide some version-control and backup functionality and do
> not require the same level of knowledge as git.

RESPONSE: In accordance with this suggestion, we have rephrased some of the text to highlight the
utility of DVCS for data analysts.

> Although DVCSs come from the world of software engineering, data analysts can significantly
> benefit from their use in the life sciences [@ram2013; @blischak2016; @perez-riverol2016].

The bullet points following this passage outline the benefits of using DVCS.
While cloud storage like dropbox could protect against code loss, they cannot other five points cannot
be satisfied.

### R3 Point 14

> 14. Page 7: Many data analysts will not know what JupyterLab or VS Code are.
References are also needed.

RESPONSE: The phrase has been updated accordingly:

> `git` is typically used at the command line, however it is also incorporated into integrated
> development environments commonly used in bioinformatics including Rstudio, JupyterLab [68,69] and
> VS Code [70,71].

### R3 Point 15

> 15. Page 9, line 7: References are missing for these other tools.

RESPONSE: Citations have been added for Podman and Apptainer/Singularity.
Rkt was removed, as it appears that development of the project has ceased.

### R3 Point 16

> 16. Page 9, lines 7-8: I believe you, but I am not aware of evidence that supports this claim.

TODO: "Docker remains the most widely used containerisation system for web development and research"

### R3 Point 17

> 17. The Biocontainers project should be mentioned.

RESPONSE: Biocontainers is mentioned in the last paragraph of the "Compute environment control" section
and has reference (Ref #91).

### R3 Point 18

> 18. Page 10, lines 51-52: I disagree that the risk is small.
> There are many instances of using genomic data to identify individuals who have committed crimes.

RESPONSE: In light of this, we have changed the section in question:

> While deidentification of data has been widely used to safeguard participant privacy, this may not be
> suitable for genomics data due to the availability of existing public genetic data sets that heighten
> reidentification risks [105].
> For example, the surnames of some male participants can be inferred based on the Y chromosome
> variants seen in deidentified genomic data together with public geneology web sites [106].
> To foster the responsible reuse of sensitive genomic and health data, the Global Alliance for
> Genomics and Health (GA4GH) initiative has proposed strategies, technical standards and policy
> frameworks designed to protect personal data in a way that perserves reproducibility [107,108].

### R3 Point 19

> 19. Figures 2 and 4 are very similar to figures used in reference 13.

Figure 4 is adapted from the diagram on the Docker official website, which was originally published in
2015 or earlier (https://web.archive.org/web/20151023003338/https://www.docker.com/what-docker).
We have cited the original source.

Figure 2 is somewhat different from reference 13, as it shows an Rstudio window with windows for raw code
in text format beside the output document.
The figure in reference 13 shows a Jupyter notebook window, which has a different arrangement of code
and outputs.

### R3 Point 20

> 20. Figure 5: I don't think it's really necessary to resummarize the FAIR principles.
> People can go to the source article for that.

RESPONSE: We acknowledge that it can sometimes be tiresome to reiterate things like the FAIR principles.
However summarising the key points succinctly will be in the interests of readers with limited time.
We note that the original description of the FAIR principles by Wilkinson and colleagues is ~5,000 words,
while this entire manuscript is ~6,500. We note that other reviewers did not insist on removing this
section.

### R3 Point 21

> 21. Page 11, line 13: I disagree that repositories like GEO and SRA are FAIR.
There are lots of problems with FAIRness in these repositories.

RESPONSE: While there may be problems with FAIRness with NCBI GEO and SRA, these repositries mostly
satisfy FAIR principles (eg: https://www.biorxiv.org/content/10.1101/739334v1.full.pdf).
From the perspective of a researcher sharing genomic data, they are "good enough" as compared to the
well known alternatives (eg: general purpose repository, institutional data storage, supplementary file,
dropbox, etc.).

### R3 Point 22

> 22. Page 11, lines 13-15: That's not true for some of these disciplines. Ecology has NEON, evo bio
has NCBI.

RESPONSE: We acknowledge that NCBI is a well known repository for evolutionary biology.
We have altered the section for clarity:

> The [re3data.org](https://www.re3data.org/) registry of data repositories may be useful to find
> FAIR repositories that accepts data from other domains of study like ecology,
> physiology, molecular simulation, social sciences and computing [@Pampel2013-ep].

### R3 Point 23

> 23. Page 11, line 34: Need evidence to back this up.

RESPONSE: Reviewer 3 commented on the passage saying supplementary data files are less
findable and accessible as compared to FAIR data repositories.
We note that Roche and colleagues previously stated that [@Roche2015-aq]:

> There are no standards for organizing supplementary data both within and across journals
> and such data are often not readily discoverable or openly accessible
> (to those without a relevant journal subscription, for example).

They also cite an article by Caetano and Aisenberg that speaks directly to the findability and
accessibility @Caetano2014-bi:

> Many journals provide the option for authors to
> include supplementary data that are made available in the digital version of the publication.
> However, repositories increase data discoverability, are more reliable for long-term storage and
> make it possible to make data sets available under open access even
> when authors transfer copyright ownership to publishers.

In light of this argument, we have added a citation to Caetano and Aisenberg in the updated text.

### R3 Point 24

> 24. Page 11, line 39: It is not necessarily true that CSV files are better than Excel.
Excel can retain information about data types, for example, whereas CSVs do not.
It depends on what you are trying to accomplish.

RESPONSE: It is true that in some cases, Excel files are more convenient to view.
However, in the context of large data sets with thousands of rows and their analytical
reproducibility, Excel is a non-starter.
For example, it is well known that the behaviour of Excel changes between versions.
Earlier versions have limitations around the number of allowed rows and columns.
Small changes to Excel files such as merging cells can break machine readibility.
Excel users often use cell highlighting or text colour to encode some information which isn't
easily machine readable.
The simplicity of TSV and CSV formats means they are readily and rapidly ingested by commonly used
programming languages without needing to install extra packages.
In line with this, the point in question has been amended:

> 5. Use file formats that are machine-readable and compatible with many different types of software.
> For example, the comma-separated values (CSV) and tab-separated values (TSV) formats are simple file
> specifications that are suitable for very large data.

### R3 Point 25

> 25. One thing that could be added is something about Common Workflow Language.
It's a community-supported specification for accomplishing many of the objectives described here.
There are some recent papers about this.

TODO:

### R3 Point 26

> 26. Page 14, lines 9-10: The big question is how to make more progress.
> We have the tools to achieve reproducibility, but why are we rarely achieving it?
> The paper mentions incentives and lack of training, which are true.
> You might consider elaborating a bit.
> My cynical view is that writing more papers and tutorials will do little without strong incentives
> and more automation.
